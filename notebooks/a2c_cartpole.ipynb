{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A2C on Cartpole\n",
    "\n",
    "In this example we will see how to train a Advantage Actor critic (A2C) agent using `torchrl`. See [Documentation](https://torchrl.sanyamkapoor.com/) for an introduction to *TorchRL* and installation instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Specification\n",
    "\n",
    "The full problem can specified in **less than 50 lines of code**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from torchrl import registry\n",
    "from torchrl import utils\n",
    "from torchrl.problems import base_hparams, A2CProblem\n",
    "from torchrl.agents import BaseA2CAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a pre-built version of the A2C agent from *TorchRL* library to initialize a `Problem`. This `Problem` class is also based on a pre-built version from the library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A2CCartpole(A2CProblem):\n",
    "  def init_agent(self):\n",
    "    observation_space, action_space = utils.get_gym_spaces(self.runner.make_env)\n",
    "\n",
    "    agent = BaseA2CAgent(\n",
    "        observation_space,\n",
    "        action_space,\n",
    "        lr=self.hparams.actor_lr,\n",
    "        gamma=self.hparams.gamma,\n",
    "        lmbda=self.hparams.lmbda,\n",
    "        alpha=self.hparams.alpha,\n",
    "        beta=self.hparams.beta)\n",
    "\n",
    "    return agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class requires us to extend the `init_agent` method. There is no restriction on the contents as long as it returns a valid `BaseAgent`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Specification\n",
    "\n",
    "We use the `HParams` object from the library to add custom properties. Again, arbitrary properties can be provided to such objects as long as they are consistently used within the previously specified `Problem` class (e.g. within the `init_agent` routine)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hparams_a2c_cartpole():\n",
    "    params = base_hparams.base_pg()\n",
    "\n",
    "    params.env_id = 'CartPole-v0'\n",
    "\n",
    "    params.num_processes = 16\n",
    "\n",
    "    params.rollout_steps = 5\n",
    "    params.max_episode_steps = 500\n",
    "    params.num_total_steps = int(1e6)\n",
    "\n",
    "    params.alpha = 0.5\n",
    "    params.gamma = 0.99\n",
    "    params.beta = 1e-3\n",
    "    params.lmbda = 1.0\n",
    "\n",
    "    params.batch_size = 128\n",
    "    params.tau = 1e-2\n",
    "    params.actor_lr = 3e-4\n",
    "\n",
    "    return params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Problem Instance\n",
    "\n",
    "We use GPUs if available and some basic arguments, most importantly the seed. Make sure to run using different seeds.\n",
    "\n",
    "**NOTE**: We use `argparse.Namespace` class as the argument to the `Problem` class which explains the type cast. If interested, track this issue [here](https://github.com/activatedgeek/torchrl/issues/61)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "args=dict(\n",
    "    seed=1,\n",
    "    log_interval=1000,\n",
    "    eval_interval=1000,\n",
    "    num_eval=1,\n",
    ")\n",
    "\n",
    "a2c_cartpole = A2CCartpole(\n",
    "    hparams_a2c_cartpole(),\n",
    "    argparse.Namespace(**args),\n",
    "    None, # Disable logging\n",
    "    device=device,\n",
    "    show_progress=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the A2C Agent\n",
    "\n",
    "Calling the `run()` routine allows us to execute training. Note that for now we have disabled logging by keeping `log_dir=None` in the above instatiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12500/12500 [05:58<00:00, 34.86epochs/s]\n"
     ]
    }
   ],
   "source": [
    "a2c_cartpole.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Training\n",
    "\n",
    "Quoting the documentation, this environment is\n",
    "\n",
    "> Considered solved when the average reward is greater than or equal to 195.0 over 100 consecutive trials.\n",
    "\n",
    "Let's see if we were able to achieve that. We use parallel environments for faster evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.2 s, sys: 221 ms, total: 1.43 s\n",
      "Wall time: 1.51 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "a2c_cartpole.agent.train(False)\n",
    "\n",
    "eval_runner = a2c_cartpole.make_runner(n_envs=10)\n",
    "eval_rewards = []\n",
    "for _ in range(100 // a2c_cartpole.runner.n_envs):\n",
    "  eval_history = eval_runner.rollout(a2c_cartpole.agent)\n",
    "  for i in range(a2c_cartpole.runner.n_envs):\n",
    "    _, _, reward_history, _, _ = eval_history[0]\n",
    "    eval_rewards.append(np.sum(reward_history, axis=0))\n",
    "eval_runner.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward: 170.5 +/- 18.714967272212903\n",
      "Did we possibly win? Nay!\n"
     ]
    }
   ],
   "source": [
    "avg_reward, std_reward = np.average(eval_rewards), np.std(eval_rewards)\n",
    "possible_win = avg_reward > 195.0\n",
    "\n",
    "print('Reward: {} +/- {}'.format(avg_reward, std_reward))\n",
    "print('Did we possibly win? {}!'.format('Yay' if possible_win else 'Nay'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_runner = a2c_cartpole.make_runner(n_envs=1)\n",
    "vis_runner.rollout(a2c_cartpole.agent,render = True)\n",
    "vis_runner.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
