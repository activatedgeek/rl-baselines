{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO on Pendulum\n",
    "\n",
    "In this example we will see how to train a Proximal policy optimization (PPO) agent using `torchrl`. See [Documentation](https://torchrl.sanyamkapoor.com/) for an introduction to *TorchRL* and installation instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Specification\n",
    "\n",
    "The full problem can specified in **less than 50 lines of code**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from torchrl import registry\n",
    "from torchrl import utils\n",
    "from torchrl.problems import base_hparams, PPOProblem\n",
    "from torchrl.agents import BasePPOAgent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a pre-built version of the PPO agent from *TorchRL* library to initialize a `Problem`. This `Problem` class is also based on a pre-built version from the library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOPendulum(PPOProblem):\n",
    "  def init_agent(self):\n",
    "    observation_space, action_space = utils.get_gym_spaces(self.runner.make_env)\n",
    "\n",
    "    agent = BasePPOAgent(\n",
    "        observation_space,\n",
    "        action_space,\n",
    "        lr=self.hparams.actor_lr,\n",
    "        gamma=self.hparams.gamma,\n",
    "        lmbda=self.hparams.lmbda,\n",
    "        alpha=self.hparams.alpha,\n",
    "        beta=self.hparams.beta,\n",
    "        max_grad_norm=self.hparams.max_grad_norm)\n",
    "\n",
    "    return agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class requires us to extend the `init_agent` method. There is no restriction on the contents as long as it returns a valid `BaseAgent`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Specification\n",
    "\n",
    "We use the `HParams` object from the library to add custom properties. Again, arbitrary properties can be provided to such objects as long as they are consistently used within the previously specified `Problem` class (e.g. within the `init_agent` routine)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hparams_ppo_pendulum():\n",
    "    params = base_hparams.base_ppo()\n",
    "\n",
    "    params.env_id = 'Pendulum-v0'\n",
    "\n",
    "    params.rollout_steps = 20\n",
    "    params.num_processes = 16\n",
    "    params.num_total_steps = int(5e6)\n",
    "\n",
    "    params.batch_size = 64\n",
    "\n",
    "    params.actor_lr = 3e-4\n",
    "\n",
    "    params.alpha = 0.5\n",
    "    params.gamma = 0.99\n",
    "    params.beta = 1e-3\n",
    "    params.lmbda = 0.95\n",
    "\n",
    "    params.clip_ratio = 0.2\n",
    "    params.max_grad_norm = 1.0\n",
    "    params.ppo_epochs = 4\n",
    "\n",
    "    return params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Problem Instance\n",
    "\n",
    "We use GPUs if available and some basic arguments, most importantly the seed. Make sure to run using different seeds.\n",
    "\n",
    "**NOTE**: We use `argparse.Namespace` class as the argument to the `Problem` class which explains the type cast. If interested, track this issue [here](https://github.com/activatedgeek/torchrl/issues/61)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "args=dict(\n",
    "    seed=1,\n",
    "    log_interval=1000,\n",
    "    eval_interval=1000,\n",
    "    num_eval=1,\n",
    ")\n",
    "\n",
    "ppo_pendulum = PPOPendulum(\n",
    "    hparams_ppo_pendulum(),\n",
    "    argparse.Namespace(**args),\n",
    "    None, # Disable logging\n",
    "    device=device,\n",
    "    show_progress=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the PPO Agent\n",
    "\n",
    "Calling the `run()` routine allows us to execute training. Note that for now we have disabled logging by keeping `log_dir=None` in the above instatiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_pendulum.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Training\n",
    "The pendulum starts in a random position, and the goal is to swing it up so it stays upright.\n",
    "Quoting the documentation, \n",
    "\n",
    "> Pendulum-v0 is an unsolved environment, which means it does not have a specified reward threshold at which it's considered solved\n",
    "\n",
    "Best 100-episode performance according to the leaderboard is -123.11 Â± 6.86\t\t\t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "ppo_pendulum.agent.train(False)\n",
    "\n",
    "eval_runner = ppo_pendulum.make_runner(n_envs=10)\n",
    "eval_rewards = []\n",
    "for _ in range(100 // ppo_pendulum.runner.n_envs):\n",
    "  eval_history = eval_runner.rollout(ppo_pendulum.agent)\n",
    "  for i in range(ppo_pendulum.runner.n_envs):\n",
    "    _, _, reward_history, _, _ = eval_history[0]\n",
    "    eval_rewards.append(np.sum(reward_history, axis=0))\n",
    "eval_runner.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_reward, std_reward = np.average(eval_rewards), np.std(eval_rewards)\n",
    "\n",
    "print('Reward: {} +/- {}'.format(avg_reward, std_reward))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_runner = ppo_pendulum.make_runner(n_envs=1)\n",
    "vis_runner.rollout(ppo_pendulum.agent,render = True)\n",
    "vis_runner.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
