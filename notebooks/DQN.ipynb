{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN on Cartpole\n",
    "\n",
    "In this example we will see how to train a DQN agent using `torchrl`. We assume basic knowledge about Reinforcement Learning and DQN. See [Documentation](https://torchrl.sanyamkapoor.com/) for an introduction to *TorchRL* and installation instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Specification\n",
    "\n",
    "The full problem can specified in **less than 50 lines of code**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from torchrl import registry\n",
    "from torchrl import utils\n",
    "from torchrl.problems import base_hparams, DQNProblem\n",
    "from torchrl.agents import BaseDQNAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a pre-built version of the DQN agent from *TorchRL* library to initialize a `Problem`. This `Problem` class is also based on a pre-built version from the library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNCartpole(DQNProblem):\n",
    "  def init_agent(self):\n",
    "    observation_space, action_space = utils.get_gym_spaces(self.runner.make_env)\n",
    "\n",
    "    agent = BaseDQNAgent(\n",
    "        observation_space,\n",
    "        action_space,\n",
    "        double_dqn=self.hparams.double_dqn,\n",
    "        lr=self.hparams.actor_lr,\n",
    "        gamma=self.hparams.gamma,\n",
    "        target_update_interval=self.hparams.target_update_interval)\n",
    "\n",
    "    return agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class requires us to extend the `init_agent` method. There is no restriction on the contents as long as it returns a valid `BaseAgent`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Specification\n",
    "\n",
    "We use the `HParams` object from the library to add custom properties. Again, arbitrary properties can be provided to such objects as long as they are consistently used within the previously specified `Problem` class (e.g. within the `init_agent` routine)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hparams_dqn_cartpole():\n",
    "    params = base_hparams.base_dqn()\n",
    "\n",
    "    params.env_id = 'CartPole-v1'\n",
    "\n",
    "    params.rollout_steps = 1\n",
    "    params.num_processes = 1\n",
    "    params.actor_lr = 1e-3\n",
    "    params.gamma = 0.99\n",
    "    params.target_update_interval = 10\n",
    "    params.eps_min = 1e-2\n",
    "    params.buffer_size = 1000\n",
    "    params.batch_size = 32\n",
    "    params.num_total_steps = 5000\n",
    "    params.num_eps_steps = 500\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Problem Instance\n",
    "\n",
    "We use GPUs if available and some basic arguments, most importantly the seed. Make sure to run using different seeds.\n",
    "\n",
    "**NOTE**: We use `argparse.Namespace` class as the argument to the `Problem` class which explains the type cast. If interested, track this issue [here](https://github.com/activatedgeek/torchrl/issues/61)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "args=dict(\n",
    "    seed=1,\n",
    "    log_interval=1000,\n",
    "    eval_interval=1000,\n",
    "    num_eval=1,\n",
    ")\n",
    "\n",
    "dqn_cartpole = DQNCartpole(\n",
    "    hparams_dqn_cartpole(),\n",
    "    argparse.Namespace(**args),\n",
    "    None, # Disable logging\n",
    "    device=device,\n",
    "    show_progress=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the DQN Agent\n",
    "\n",
    "Calling the `run()` routine allows us to execute training. Note that for now we have disabled logging by keeping `log_dir=None` in the above instatiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:14<00:00, 301.06epochs/s]\n"
     ]
    }
   ],
   "source": [
    "dqn_cartpole.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Training\n",
    "\n",
    "Quoting the documentation, this environment is\n",
    "\n",
    "> Considered solved when the average reward is greater than or equal to 195.0 over 100 consecutive trials.\n",
    "\n",
    "Let's see if we were able to achieve that. We use parallel environments for faster evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 35.2 s, sys: 2.5 s, total: 37.7 s\n",
      "Wall time: 39.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "dqn_cartpole.agent.train(False)\n",
    "\n",
    "eval_runner = dqn_cartpole.make_runner(n_envs=10)\n",
    "eval_rewards = []\n",
    "for _ in range(100 // dqn_cartpole.runner.n_envs):\n",
    "  eval_history = eval_runner.rollout(dqn_cartpole.agent)\n",
    "  for i in range(dqn_cartpole.runner.n_envs):\n",
    "    _, _, reward_history, _, _ = eval_history[0]\n",
    "    eval_rewards.append(np.sum(reward_history, axis=0))\n",
    "eval_runner.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward: 273.17 +/- 142.67628078976549\n",
      "Did we possibly win? Yay!\n"
     ]
    }
   ],
   "source": [
    "avg_reward, std_reward = np.average(eval_rewards), np.std(eval_rewards)\n",
    "possible_win = avg_reward > 195.0\n",
    "\n",
    "print('Reward: {} +/- {}'.format(avg_reward, std_reward))\n",
    "print('Did we possibly win? {}!'.format('Yay' if possible_win else 'Nay'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_runner = dqn_cartpole.make_runner(n_envs=1)\n",
    "vis_runner.rollout(dqn_cartpole.agent,render = True)\n",
    "vis_runner.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prioritized DQN on Cartpole\n",
    "\n",
    "In this example we will see how to train a Prioritized DQN agent using `torchrl`. We assume basic knowledge about Reinforcement Learning and Prioritized DQN. See [Documentation](https://torchrl.sanyamkapoor.com/) for an introduction to *TorchRL* and installation instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Specification\n",
    "\n",
    "The full problem can specified in **less than 50 lines of code**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from torchrl import registry\n",
    "from torchrl import utils\n",
    "from torchrl.problems import PrioritizedDQNProblem\n",
    "from torchrl.agents import BaseDQNAgent\n",
    "\n",
    "from cartpole_v1 import DQNCartpole"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a pre-built version of the DQN agent from *TorchRL* library to initialize a `Problem`. This `Problem` class is also based on a pre-built version from the library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PERCartpole(PrioritizedDQNProblem):\n",
    "  def init_agent(self):\n",
    "    observation_space, action_space = utils.get_gym_spaces(self.runner.make_env)\n",
    "\n",
    "    agent = BaseDQNAgent(\n",
    "        observation_space,\n",
    "        action_space,\n",
    "        double_dqn=self.hparams.double_dqn,\n",
    "        lr=self.hparams.actor_lr,\n",
    "        gamma=self.hparams.gamma,\n",
    "        num_eps_steps=self.hparams.num_eps_steps,\n",
    "        target_update_interval=self.hparams.target_update_interval)\n",
    "\n",
    "    return agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class requires us to extend the `init_agent` method. There is no restriction on the contents as long as it returns a valid `BaseAgent`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Specification\n",
    "\n",
    "We use the `HParams` object from the library to add custom properties. Again, arbitrary properties can be provided to such objects as long as they are consistently used within the previously specified `Problem` class (e.g. within the `init_agent` routine)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hparams_per_cartpole():\n",
    "    params = DQNCartpole.hparams_dqn_cartpole()\n",
    "\n",
    "    params.alpha = 0.6\n",
    "    params.beta = 0.4\n",
    "    params.beta_anneal_steps = 1000\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Problem Instance\n",
    "\n",
    "We use GPUs if available and some basic arguments, most importantly the seed. Make sure to run using different seeds.\n",
    "\n",
    "**NOTE**: We use `argparse.Namespace` class as the argument to the `Problem` class which explains the type cast. If interested, track this issue [here](https://github.com/activatedgeek/torchrl/issues/61)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "args=dict(\n",
    "    seed=1,\n",
    "    log_interval=1000,\n",
    "    eval_interval=1000,\n",
    "    num_eval=1,\n",
    ")\n",
    "\n",
    "per_cartpole = PERCartpole(\n",
    "    hparams_per_cartpole(),\n",
    "    argparse.Namespace(**args),\n",
    "    None, # Disable logging\n",
    "    device=device,\n",
    "    show_progress=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Prioritized DQN Agent\n",
    "\n",
    "Calling the `run()` routine allows us to execute training. Note that for now we have disabled logging by keeping `log_dir=None` in the above instatiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:36<00:00, 277.02epochs/s]\n"
     ]
    }
   ],
   "source": [
    "per_cartpole.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Training\n",
    "\n",
    "Quoting the documentation, this environment is\n",
    "\n",
    "> Considered solved when the average reward is greater than or equal to 195.0 over 100 consecutive trials.\n",
    "\n",
    "Let's see if we were able to achieve that. We use parallel environments for faster evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.4 s, sys: 1.29 s, total: 15.7 s\n",
      "Wall time: 16.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "per_cartpole.agent.train(False)\n",
    "\n",
    "eval_runner = per_cartpole.make_runner(n_envs=10)\n",
    "eval_rewards = []\n",
    "for _ in range(100 // per_cartpole.runner.n_envs):\n",
    "  eval_history = eval_runner.rollout(per_cartpole.agent)\n",
    "  for i in range(per_cartpole.runner.n_envs):\n",
    "    _, _, reward_history, _, _ = eval_history[0]\n",
    "    eval_rewards.append(np.sum(reward_history, axis=0))\n",
    "eval_runner.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward: 127.04 +/- 9.356195808126293\n",
      "Did we possibly win? Nay!\n"
     ]
    }
   ],
   "source": [
    "avg_reward, std_reward = np.average(eval_rewards), np.std(eval_rewards)\n",
    "possible_win = avg_reward > 195.0\n",
    "\n",
    "print('Reward: {} +/- {}'.format(avg_reward, std_reward))\n",
    "print('Did we possibly win? {}!'.format('Yay' if possible_win else 'Nay'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_runner = per_cartpole.make_runner(n_envs=1)\n",
    "vis_runner.rollout(per_cartpole.agent,render = True)\n",
    "vis_runner.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
